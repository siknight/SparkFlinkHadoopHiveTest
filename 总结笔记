idea常用快捷键总结:
    ctrl+p提示参数快捷键
    ctrl+h显示子类的实现
    main方法：
        scala：直接写main会提示main方法
        java：写psvm会提示main方法
     生成变量：
        scala定义完对象后，加上.var 会生成变量名
        java：使用ctrl + alt + v显示变量
liunx软件安装：
    mysql需要rpm，其他的直接解压
    alt+p打开linux的sftp，可以传文件
hadoop总结:（不用配zookeeper，除了HA）
    1.hdfs是分布式存储框架
        （1）hadoop= hdfs + yarn + mapreduce + common
            命令hdfs dfs -命令（用hdfs dfs可以查看所有命令）
                  或者 hadoop fs -命令（用hadoop fs可以查看所有命令）
        （2）连接到hdfs的url: hdfs://hadoop102:9000
             连接到hdfs的web路径 http://hadoop102:50070
             连接到yarn的web路径 http://hadoop102:8088
         (3)代码操作案例：
                 //1.首先连接到hdfs系统客户端
                 FileSystem fs = FileSystem.get(
                    new URI("hdfs://hadoop102:9000"), new Configuration(), "lisi");
                  //2.然后用fs客户端操作
                 fs.mkdirs(new Path("/lisi/lisi07"));
                ----------------------------------
            代码操作用法和hdfs程序很相似，概括就是先获取客户端，然后用获取的客户端操作
          （4）
                hdfs由NameNode，DataNode和Secondary NameNode组成
                一，其中NameNode相当于是数据的目录，是存储数据的元数据
                    如文件名，文件目录，文件属性，以及每个文件得块列表
                    和快的DataNode等
                二，DataNode存储真正的数据和块数据的校验和
                三,Secondary NameNode 用来监控hdfs的辅助后台程序，每隔一段时间
                    获取hdfs元数据的快照。


    2.mapreduce是分布式计算框架
        （1）
            不擅长实时计算，不擅长流式计算，数据源必须是静态的
        （2）
            Mapper阶段：
                map()方法（MapTask进程）对每一个<k,v>调用一次
            Reducer阶段：
                reduceTask进程对每一组相同k的<k,v>组调用一次reduce()方法
        （3）
            FileInputFormat切片机制：
                每一个Split切片分配一个MapTask并行实时实例处理，也就是
            有多少个split切片就有多少个MapTask。默认情况下切片大小=BlockSize。
            其次切片时不考虑不考虑数据集集体，而是针对每一个文件单独切片。
                比如：a.txt：200M  b.txt 10M
                    a.txt分成：0-128M和128-200M两个分片 。 b.txt 会分成0-10M一个分片。
                    所以共分成3个分片，也就是3个mapTask
          (4)
               提交jar方法 hadoop jar jar包路径  main类 输入路径 输出路径（要不存在，否则报错）
                    注意：如果开启了hdfs，那输入路径和输出路径直接用/就表示是hdfs的路径
                在idea里配置运行时，首先点击edit configuration，
                然后在program parameter里配置输入路径和输出路径，参数之间用空格分开，
                本地直接写相对路径和绝对路径就行，
                如果是hdfs，就写：hdfs://hadoop102:9000/输入路径 hdfs://hadoop102:9000/输出路径
     3.hadoop配置方式（看着word文档直接配，很简单）
        （1）本地模式：直接解压就能用，配置一台机器
        （2）伪分布式模式：配置一台机器,需要配置namenode主节点，yarn主节点，
         (3)完全分布式：配置三台机器，需要配置namenode主节点，yarn主节点，
            和在slave里配置上三台机器
      4.yarn（hadoop2.0以后才有）
         （1）2.0后yarn只负责资源的调度，MapReduce只负责运算。
                1.0时代MapReduce同时负责资源调度和业务处理，耦合性较大
          （2）yarn包括以下四部分：
               一，ResourceManager：（资源的分配，相当于盖房子的大包头，监控用户，小包头和手下的监工和分活的）
                    01.处理客户端请求
                    02.监控NodeManager
                    03.启动和监控ApplicationMaster
                    04.负责资源的分配和调度（问datanode是否有空）
                二，NodeManager（和单个节点资源管理有关，相当于公司各个小包头）
                    01.管理单个节点上的资源
                    02.处理来自ResourceManager上的命令
                    03.处理来自ApplicationMaster上的命令
                 三，ApplicationMaster（和任务运行有关，相当于大包头拍下来监工和应用分工的（给datanode分配资源））
                      01.负责数据的切分
                      02.任务的监控和容错
                      03.为应用程序请求资源并给内部处理
                      03.任务的监控与容错
                  四，Container（单个节点资源抽象，相当于一个每个小包头的工作资源，如人力，干活工具）
                     （1）Container是yarn上资源的抽象
                     （2）它封装了某个节点多维度资源，
                            如内存，CPU,硬度，网络等。

                    我认为整个流程可以简单理解为，客户端首先将job任务提交到ResourceManager，
                  然后ResourceManager管理资源，和一些NodeManager单个节点资源管理者通信，
                  问他们是否可以运行这个job，如果可以，ResourceManager就让applicationMaster给NodeManager分配
                    任务，然后NodeManager拿到分配任务后，就利用手上的container资源干活。
                    同时Container干活受到ApplicationMaster的监控。


zookeeper总结：
    1.zookeeper是分布式服务管理框架
                 zookeeper = 文件存储+通知机制。
    2.通过zkServer.sh start 进入服务器端，如果是完全分布式那么三个节点都要打开此命令

    3.用zkCli.sh进入shell窗口，进入shell窗口后可以用help查看所有指令，然后很简单

    4.
        一.首先用ZooKeeper zkClient =new ZooKeeper(param1,param2,param3)获取客户端
        二.用如zkClient.delete("/fuxi/lisi02",-1)这样的操作操作zookeeper
           用法和hdfs程序很相似，概括就是先获取客户端，然后用获取的客户端操作


hive总结：

hbase总结


scala语言总结：
    1.安装（安装方法和jdk很相似）
        （1）安装scala语言之前必须先安装jdk，推荐jdk1.8，scala在java虚拟机上运行
         (2)安装步骤
            （scala官网：https://www.scala-lang.org/）
            一,配置SCALA_HOME= D:\scala-2.11.8
            二，在PATH变量中添加：%SCALA_HOME%\bin
            三，打开cmd命令，输入scala打开scala解释器
            四，在idea工具上运行时需要在idea上安装插件
    2.只有object修饰的类里才能放main方法。以下是案例：
        java：
           public class HelloJava {
               public static void main(String[] args) {

               }
           }

       scala：

           object MainTest {

             def main(args: Array[String]) {
               //后面的;可以省略
               println("first scala main method");
             }

           }

    3.简介：
        （1）scala是一门多范式编程语言，其中的范式指编程方式
            （比如scala融合了函数式编程和面向对象编程），
            不同的编程语言，编程方式也不同，也就意味着scala融合了
             多种不同编程方式的语言。所以Scala编程的初衷是集成面向对象
            和函数式编程的各种特性
        （2）Scala源代码(.scala)会被编译成字节码（.class），
        然后运行于JVM上，并可以调用java的类库，实现两种语言的无缝对接。

          项目里的target目录也用于存放程序运行后的.class文件，可以用jd-gui软件打开
    4.函数式编程：（函数式编程过程可以对比js代码学习）
         （1）它属于“结构化”编程的一种，主要思想是把运算过程写成一系列
       嵌套的函数调用。
         （2）函数编程语言最重要的基础是λ演算，而且λ演算的函数可以接受
         函数作为输入（参数）和输出（返回值）
          （3）几种语言区别：
                一，scala，java和js一样用{}作为代码块，python语言不同
                用：和空格区别代码块。
                二，scala和python都用def修饰方法。
                三，scala和js都用var修饰变量，区别scala还可以用
                     val表示不变量。python没有修饰变量的，直接写 a =10就行
                 四，scala，python和java是强类型语言，有返回值，js无返回值。
                       scala：
                            def  f1(): Unit ={
                                println("hello world");
                              }

                              def  f2(a:Int,b:Int) ={
                                a+b;
                              }
                       java：
                           public int sum（int a,int b）{
                                return a+b;
                           }

                        python:
                            def functionname( parameters ):
                               return [expression]

                 五.参数区别
                     python直接写变量就行，java是（int a），scala是
                      （a:Int）,如果方法没有参数，小括号可以省略。


              （4）Unit表示无返回值，和java的void一样。


              （5）方法相关
                   一，方法格式
                     def 函数名（参数名[：参数类型]…）[: 返回值类型 = ] {
                        函数体
                     }
                     解释：返回值类型为整个方法的返回值类型
                    二，关于返回值的几点说明
                      *如果函数无返回值，那么返回值类型无需声明
                      *如果函数有返回值，但返回值类型没有声明。
                      方法返回值Scala会自动推断
                     （函数方法会使用最后一行的结果作为函数的返回值,
                        Println函数的返回值是unit）
                      *如果函数有返回值，且使用return声明的，
                      那么返回值类型需要声明（即使有unit，矛盾了，但是也不会报错
                      但是return不生效，函数的结果就是括号）
                      *如果函数无参数列表，那么小括号可以省略
               （6） 变量
                    一，格式：var | val 变量名 [：变量类型] = 变量值
                        其中[]表示可要可不要
                        var关键字表示声明的值可以改变，
                        而val关键字声明的变量表示值无法改变
                    二，scala中所有的类型都是首字母大写的，后面可以跟任何符号
                    三，一行逻辑代码的最后，无需使用“；”但是一行中
                        如果存在多个逻辑代码，必须使用“；”分开。
                    四，在函数式编程中，函数与其他数据类型一样，处于平
                      等地位，可以赋值给其他变量，也可以作为参数，传入另一个函数
                      或者作为别的函数的返回值。（记住和变量使用方法一样就行）
       5.面向对象（可以参考java）
                     一，类对比
                        java：
                           *格式
                               [修饰符] class 类名 {
                                    类体
                               }
                            *java语法中要求一个java源码文件中可以声明多个类，
                             但是公共类只能有一个，且必须和源码文件的文件名保持一致。
                        Scala：
                            *格式
                                 [修饰符] class|object 类名 {
                                    类体
                                 }
                             *scala语法中，类并不声明为public，
                             一个Scala源文件可以包含多个类。所有这些类都具有公有可见性
                      二，对象对比
                               java：
                                    格式
                                       // 类型 变量 = new 类型();
                                       // User user = new User();
                                       // Java是强类型语言，声明任何变量的同时都必须声明类型
                                scala:
                                    格式
                                        val 变量[:类型] = new 类型();
                                        val user:User = new User();
                                        scala在声明对象变量时，可以根据创建对象的类型进行推断
                                        所以类型声明可以省略
                        三，属性对比
                            java：
                                  * private 类型 属性名称；
                                  *public Setter/Getter方法
                                   *Java语法中如果声明了一个属性，
                                   但是没有显示的初始化，那么会给这个属性默认初始化
                                   *通过setter/getter方法对属性进行赋值和取值
                                    setter/getter方法一般都是公共访问权限，起到了封装的作用
                             scala:
                                  *  private var 属性名称 [：类型] = 属性值
                                  * def Setter/Getter方法
                                  *Scala中声明一个属性,必须始化，然后根据初始化数据的类型自动推断，属性类型可以省略，
                                      如果初始化的值设定为null，也可以使用符号_(下划线)代替
                                  *Scala为了访问一致性，所以并不推荐由开发人员自行定义setter/getter方法,
                                       Scala中为了简化代码的开发，当声明属性时，本身就自动提供了对应的setter/getter方法
                                  *如果属性声明为private的，那么自动生成的setter/getter方法也是private的
                                        如果属性省略访问权限修饰符，那么自动生成的setter/getter方法是public的
                         四 普通方法对比
                                java
                                    *public 返回值类型 方法名(参数列表) { 方法体 }
                                                                         *调用方法 对象.方法名()
                                                                     scala
                                    *def 方法名(参数列表) [：返回值类型] = {方法体}
                                    *Scala中的方法其实就是函数，只不过一般将对象中的函数称之为方法
                                    *调用方法  对象.方法名()
                          五.静态方法对比
                                java(加static关键字即可)
                                    *public static 返回值类型  方法名(参数列表) {方法体}
                                    *java中静态方法并不是通过对象调用的，而是通过类对象调用的，
                                        所以静态操作并不是面向对象的。
                                 scala(样例代码还没有)
                                      *Scala语言是完全面向对象(万物皆对象)的语言，所以并没有静态的操作。
                                        但是为了能够和Java语言交互，就产生了一种特殊的对象来模拟类对象，
                                        我们称之为类的伴生对象。这个类的所有静态内容都可以放置
                                        在它的伴生对象中声明和调用
                                      *Scala中伴生对象采用object关键字声明，
                                       伴生对象中声明的全是“静态”内容，可以通过伴生对象名称直接调用。
                                       *伴生对象对应的类称之为伴生类，伴生类和伴生对象应该在同一个源码文件中
                                       *从语法角度来讲，所谓的伴生对象其实就是类的静态方法和成员的集合
                                       * 从技术角度来讲，所谓的伴生对象在编译时，
                                       会将对应的代码以静态的方式生成到类的字节码中。
                              六，包
                                    java：
                                        这个我们都知道
                                     scala：
                                        *scala语言也可以使用包管理类
                                        *scala 包名和源码实际的存储位置没有关系
                                        *从技术角度来讲，Scala的编译器会将Scala中的包编译成符合Java语法规则的包结构
                              七，继承
                                    java:
                                        class 子类名 extends 父类名 { 类体 }
                                        子类继承父类的属性和方法
                                     scala
                                         class 子类名 extends 父类名 { 类体 }
                                         子类继承父类的属性和方法

                               八，接口
                                    java:
                                        声明接口
                                        interface 接口名
                                        实现接口
                                       在Java中，接口并不属于类的体系关系，所以可以多实现，且接口支持多继承
                                        接口中属性都是常量
                                        接口中的方法都是抽象的
                                     scala:
                                        *Scala语言中，采用特质（特征）来代替接口的概念，也就是说，
                                            多个类具有相同的特征（特征）时，
                                            就可以将这个特质（特征）独立出来，采用关键字trait声明
                                         *声明特质 trait 特质名
                                         *一个类具有某种特质（特征），就意味着这个类满足了这个特质（特征）的所有要素，所以在使用时，
                                         也采用了extends关键字，如果有多个特质或存在父类，
                                         那么需要采用with关键字连接
                                         * class类名 extends 特质1名称 with 特质2名称 with 特质3名称
                    6，标识符：
                          scala首字符为操作符，后续字符为任意操作符

                    7，val定义的值是不可变的，它不是一个常量，
                        是不可变量，或称之为只读变量。
                    8.类型
                        *Scala语言是完全面向对象的语言，
                            所以并不区分基本类型和引用类型，
                            这些类型都是对象，我们称之为常用类型。
                        *Scala常用类型中包含有7种数值类型：
                        Byte、Char、Short、Int、Long、Float、Double
                        *因为这些类型都是对象，所以他们都可以调用方法
                         如 1.toString

                    9.
                        （1）Scala中，所有的值都是类对象，而所有的类，包括值类型
                    ，都最终继承自一个统一的根类型Any。统一类型，是Scala的
                    又一大特点。
                        （2）Scala中还定义了几个底层类（Bottom Class），
                            比如Null和Nothing
                                *Null是所有引用类型的子类型，而Nothing是所有类型的
                                子类型。Null类只有一个实例对象，null，类似于java中
                                的null引用。null可以赋值给任意引用类型，但是不能赋值
                                给值类型。
                                *Nothing，可以作为没有正常返回值的方法的返回类型，
                                非常直观的告诉你这个方法不会正常返回。而且由于Nothing是
                                其他任意类型的自雷，他还能跟要求返回值的方法兼容。
                 10.Scala中没有++、--操作符，需要通过+=、-=来实现同样的效果。

                 11.Scala认为任何表达式都会有值，对于空值，使用Unit类，叫做无用占位符

                 12.Scala中没有三元运算符

                 13与If语句不同，While语句本身没有值，即整个While语句的结果是Unit类型的()。

                 14因为while和do…while中没有返回值,所以外部定义while的变量不推荐使用

                 15Scala内置控制结构特地去掉了break和continue，是为了更好的适应函数化编程，
                   推荐使用函数式的风格解决break和contine的功能，而不是一个关键字。
                     示例代码如下：
                    import util.control.Breaks._
                    breakable{  //带breakable只会带包含的代码，如果不带遇到break（）后会中断后面所有的代码
                      while(n <= 20){
                        n += 1;
                        if(n == 18){
                          break()
                        }
                      }
                    }


                    用下面这个就行；
                        object BreakTest {

                          def main(args: Array[String]): Unit = {
                            var i =1;
                            while (i<=10){
                              if (i==5){
                                Breaks.break();
                              }else{
                                println(i)
                                i=i+1;
                              }
                            }
                          }

                        }

                      16 for方法很不同，这里的for循环又被称为for表达式和for推导式
                            （1）to左右两边为前闭后闭的访问
                                        for(i <- 1 to 3){
                                          print(i + " ")
                                        }
                             （2）until左右两边为前闭后开的访问
                                    for(i <- 1 until 3) {
                                      print(i + " ")
                                    }
                             （3）循环守卫
                                    for(i <- 1 to 3 if i != 2) {
                                      print(i + " ")
                                     }

                             （4）引入变量
                                for(i <- 1 to 3; j = 4 - i) {
                                    print(j + " ")
                                  }
                              （5）嵌套循环：
                                    for(i <- 1 to 3; j <- 1 to 3) {
                                      print(i*j + " ")
                                    }
                               （6）循环返回值：
                                    （将遍历过程中处理的结果返回到一个新集合中，
                                            使用yield关键字）
                                val for5 = for(i <- 1 to 10) yield i
                                （7）使用花括号{}代替小括号()：
                                    for{
                                      i <- 1 to 3
                                      j = 4 - i
                                    }
                                     注：{}和()对于for表达式来说都可以。
                                     for 推导式有一个不成文的约定：
                                     当for 推导式仅包含单一表达式时使用原括号
                                     ，当其包含多个表达式时使用大括号。
                                     值得注意的是，使用原括号时，
                                     早前版本的Scala 要求表达式之间必须使用分号。
                     (8)函数
                        一，格式
                         def 函数名(参数名1: 参数类型1, 参数名2: 参数类型2) : 返回类型 = {
                            函数体
                         }
                        二，注：如果函数明确声明无返回值（声明Unit）
                          ，那么函数体中即使使用return关键字也不会有返回值。
                          如果明确函数无返回值或不确定返回值类型，那么返回值类型可以省略
                         三，：函数如果参数列表不为空，
                             那么在调用时需要传递参数，不能省略，
                             这和JavaScript语法不一样，在JavaScript中调用有参函数时，
                             如果没有传递参数，那么参数会自动赋值为undefined,
                              Scala也可以实现类似的功能，就是在声明参数时，直接赋初始值。
                                  def f5(p:String = "f5") {
                                      println(p);
                                  }
                          四，如果函数存在多个参数，每一个参数都可以设定默认值
                          ，那么这个时候，传递的参数到底是覆盖默认值，
                          还是赋值给没有默认值的参数，就不确定了(默认按照声明顺序)。
                          在这种情况下，可以采用带名参数
                                def f6 ( p1 : String = "v1", p2 : String ) {
                                    println(p1 + p2);
                                }
                                f6("v2" )  // (X)
                                f6(p2="v2") // (OK)

                          五， 变长参数（不确定个数参数，类似Java的...）
                            def f7(args: Int*) = {
                              var result = 0
                              for(arg <- args)
                                result += arg
                              result
                            }
                          六，递归函数（重点）
                                // 递归函数未执行之前是无法推断出来结果类型，在使用时必须有明确的返回值类型
                                def f8(n: Int): Int = {
                                  if(n <= 0)
                                    1
                                  else
                                    n * f8(n - 1)
                                }
                            七， 过程：
                                    将函数的返回类型为Unit的函数称之为过程。
                                    如果明确函数没有返回值，那么等号可以省略
                                     def f9(content: String) = {
                                       println(“f7”)
                                     }
                                     def f9(content: String) { // 明确无返回值
                                       println(content)
                                     }

                                注：开发工具的自动代码补全功能，虽然会自动加上Unit，
                                但是考虑到Scala语言的简单，灵活，能不加最好不加

               八，惰性函数：当函数返回值被声明为lazy时，
                       函数的执行将被推迟，直到我们首次对此取值。这种函数我们称之为惰性函数
                       ，在Java的某些框架代码中称之为懒加载（延迟加载）
                          def f10(): String = {
                            println("f10方法执行")
                          }
                          lazy val msg = f10()
                          println("f10方法没有执行")
                          println(msg)
                九，异常
                       object ExceptionTest {

                         def main(args: Array[String]): Unit = {
                           try{
                               var a =10/0;
                           }catch {
                             case ex:ArithmeticException =>{
                               print("发生了算术异常")
                             }
                             case ex:Exception =>print("发生了异常");
                           }
                         }

                       }
               十，
                 scala中的classOf[NumberFormatException] 相当于 java中的NumberFormatException.class

               十一，
                        标识符，首字母为运算符（+-*/）,后面字少在跟一个
                        比如 val ++ =123
                十二，如果函数体中只有一行代码，大括号可以省略（重点）
                  如：def test() = "aaaa";
                     //如果定义方法时带了（）且是无参的，那么调用时（）可以省略
                      def test = "aaa";
                       //如果定义方法时小括号（）省略，则调用时不能带（），直接test就行

                 十三，scala里的变量和方法的类型都可以省略不写，其中方法如果明确告诉此方法没有
                 返回值类型，也就是方法返回值是Unit类型的，那么={}的=可以省略不写。（重点）

                 十四，匿名函数
                   （）.{println("xxxxx")}

                  十五.
                    输出时$符可以直接在输出时拼接
                        def bb(a:Int,b:Int) ={
                           println(s"${a}*****${b}");
                         }

                   十六.
                       *如果主构造器无参数，小括号可省略
                            例如
                             class ConstructorClass2() {
                               var a="hello";
                             }
                             val clazz = new ConstructorClass
                        *辅助构造器4)名称为this，
                            多个辅助构造器通过不同参数列表进行区分
                         *在类的属性上加@BeanProperty会自动生成get和set方法，但是如果
                         不是抽象类，属性必选要给一个初始值
                             @BeanProperty
                             var name:String ="hehe"
                             @BeanProperty
                             var age:Int =22
                        *如果想让主构造器变成私有的，
                            可以在()之前加上private，这样用户只能通过辅助构造器来构造对象了

                        *类对象，伴生类，伴生对象
                                注：类和它的伴生对象可以相互访问私有属性或方法，
                                他们必须存在同一个源文件中。必须同名。
                                // person.scala
                                object Person{ // 类对象
                                }
                                // person.scala
                                class Person { // 伴生类
                                }
                                object Person { // 伴生对象
                                }
                                从技术的角度来讲，伴生对象和伴生类其实是两个不同的类，
                                伴生对象所对应的类可以简答的理解为伴生类的辅助工具类。
                                而伴生对象就是这个辅助工具类的单例对象，专门用于处理伴生类中静态资源的相关功能.
                                专门用于处理伴生类中静态资源的相关功能.
                                伴生对象既然是一个单例对象，就意味着类已经声明过了，
                                所以，伴生（类）对象是不能声明构造器的。
                                伴生对象既然是一个单例对象，就意味着类已经声明过了，所以，伴生（类）对象是不能声明构造器的
                 十七.辅助构造器名称为this（这个和Java是不一样的），
                        多个辅助构造器通过不同参数列表进行区分
                        class Person() {
                            private var sname = ""
                            private var iage = 0
                            // 辅助构造器无论是直接或间接，最终都一定要调用主构造器，执行主构造器的逻辑
                            def this( name : String ) {
                                this() // 放在第一行
                                sname = name
                            }
                            def this( name : String, age : Int ) {
                                this(name) //调用之前已经声明过该构造器
                                iage = age
                            }
                        }
               十八.如果想让主构造器变成私有的，可以在()之前加上private，
                   这样用户只能通过辅助构造器来构造对象了
                   class ConstructorClass private () {
                   }
                   val obj = new ConstructorClass() // (X)

               十九.apply方法
                    因为伴生对象可以处理静态资源，
                    所以完全可以通过伴生对象提供的方法对伴生类进行处理
                    object Person {
                        def apply() =  { // 在伴生对象中实现apply方法。
                            return new Person() // 这里也可以采用单例模式实现
                        }
                    }
                    …
                    val p = Person() // 此处不需要使用new关键字，
                    而是直接增加小括号，在执行时，
                    会自动调用伴生对象中的apply方法。返回伴生类实例对象
                二十.应用程序对象
                    每一个Scala应用程序都需要从一个对象的main方法开始执行，
                     这个方法的类型为Array[String]=>Unit：
                     object ScalaApp{
                       def main(args: Array[String]) {
                         println("Hello, Scala!")
                       }
                     }
                     或者扩展一个App特质：
                     object ScalaApp extends App {
                       println("Hello, Scala!")
                     }

                二十一.枚举对象
                    Scala中没有枚举类型，定义一个扩展Enumeration类的对象，
                    并以Value调用初始化枚举中的所有可能值,模拟枚举类型。
                    object LightColorEnum extends Enumeration {
                      val Red = Value(0, "Stop")
                      val Yellow = Value(1, "Slow")
                      val Green = Value(2, "Go")
                    }
                二十二.构造器参数
                    *Scala类的主构造器函数是可以添加参数的。
                    如果参数未用任何修饰符修饰，那么这个参数是局部变量
                        var p = new Person("lisi")
                        println(p.name) (X)
                    *如果参数使用val关键字声明，
                    那么Scala会将参数作为类的只读属性（不能修改）使用
                            class Person( val name : String ) {
                            }
                            var p = new Person("lisi")
                            println(p.name) (OK) // 这里调用对象的name属性，
                                其实并不是属性，而是方法，因为方法无参数，
                                所以省略了小括号，感觉和调用属性一样，
                                这体现了Scala语言访问一致性
                    *如果参数使用var关键字声明，
                        那么那么Scala会将参数作为类的成员属性使用,
                        并会提供属性对应的setter/getter方法
                            class Person( var name : String ) {
                            }
                            var p = new Person("lisi")
                            p.name = "wangwu" // setter
                            println(p.name)  //getter
           二十三.在scala中，访问权限分为：
                 public, private,protected, 缺省的
               *var name = "zhangsan" // 此时属性为public访问权限
                        ,任何地方都能访问
               *private var name = "zhangsan"  // 此时属性为私有的，
                                只在类的内部和伴生对象中可用
                *protected var name = "zhangsan"  // scala中受保护权限比Java中更严格，
                                    只能子类访问，同包无法访问
                 *4)包访问权限
                 private [scala] val pname="zhangsan" // 增加包访问权限后，
                        private同时起作用
                   当然，也可以将可见度延展到上层包：

                   private[atguigu] val description="zhangsan"
            二十四.因为Scala语言源自于Java，
            所以java.lang包中的类会自动引入到当前环境中，
            而Scala中的scala包和Predef包的类也会自动引入到当前环境中。
            如果想要把其他包中的类引入到当前环境中，需要使用import语言

            二十五.
                *Scala中采用下划线作为通配符
                 import scala.beans._ // 这里采用下划线作为通配符
                 *如果不想要某个包中全部的类，而是其中的几个类，可以采用选取器（大括号）
                    import scala.collection.mutable.{HashMap, HashSet}
                  *如果引入的多个包中含有相同的类，那么可以将不需要的类进行重命名进行区分
            二十六
            .注：位于文件顶部不带花括号的包声明在
                 对当前整个文件内的包声明有效。
            二十七.
                *当访问权限缺省时，scala默认为public访问权限
                  class Person {
                      var name = "zhangsan" // 此时属性为public访问权限,任何地方都能访问
                  }
                *私有权限
                    class Person {
                        private var name = "zhangsan"
                        // 此时属性为私有的，只在类的内部和伴生对象中可用
                    }
                *受保护权限
                    class Person {
                         protected var name = "zhangsan"
                         // scala中受保护权限比Java中更严格，只能子类访问，同包无法访问
                     }
                 *包访问权限
                    package com.atguigu.scala
                    class Person {
                      private[scala] val pname="zhangsan" // 增加包访问权限后，private同时起作用
                    }
                    当然，也可以将可见度延展到上层包
                    private[atguigu] val description="zhangsan"

         二十八.继承
              *重写方法
                重写一个非抽象方法需要用override修饰符，
                  调用超类的方法使用super关键字
              *类型检查和转换
                 1)  classOf[String]就如同Java的 String.class
                 2)  obj.isInstanceOf[T]就如同Java的obj instanceof T
                 3)  obj.asInstanceOf[T]就如同Java的(T)obj（强制转换)
         二十九.抽象类和匿名内部类
            abstract  class Person2 { // 抽象类
              var name:String;  //  抽象字段,属性未初始化时，就是抽象的
                                //用抽象字段时，class前面必须带 abstract
              def hello; //抽象方法

            }

            class Emp2 extends Person2{
               var name: String = _ //注：子类重写抽象方法不需要override
              //注：子类重写抽象方法不需要override
               def hello: Unit = ???
            }

              //匿名内部类
                val person: Person2 = new Person2 {
                  override var name: String = _

                  override def hello: Unit = ???
                }

           三十.特质
               *
                trait Logger{
                  def log(msg:String);
                }

                class Console extends Logger with Cloneable with Serializable{
                  override def log(msg: String): Unit = ???
                }
            *和Java中的接口不太一样的是特质中的方法
                并不一定是抽象的，也可以有默认实现
                     trait Logger{
                       def log(msg:String): Unit ={
                         println("你好")
                       }
                     }

                     class Console extends Logger with Cloneable with Serializable{
                        //当trait里有具体实现时，
                         //这个方法不用被子类继承

                     }
            *动态混入
               trait DBOperate {
               def insert( id : Int ) {
                   println("插入主键【"+id+ "】数据" )
               }
               }
               class MySQL {
               }
               var db = new MySQL with DBOperate
               db.insert(1)

        三十一.隐式转换和隐式参数
            *隐式转换函数是以implicit关键字声明的带有单个参数的函数。
             这种函数将会自动应用，将值从一种类型转换为另一种类型。
             *需要保证在当前环境下，只有一个隐式函数能被识别
                 def main(args: Array[String]): Unit = {

                     //声明隐式函数
                    implicit  def toInt(d:Double)=d.toInt;

                    var i:Int = 3.5;
                    println(i)
                  }
             *隐式类注意点
                1)其所带的构造参数有且只能有一个
                2)隐式类必须被定义在“类”或“伴生对象”或“包对象”里
                3)隐式类不能是case class（case class在后续介绍）
                4)作用域内不能有与之相同名称的标示符
                    object StringUtils {
                    //隐式类,必须有有一个参数的主构造函数
                      implicit class StringImprovement(val s : String){ //隐式类
                        def addSuffix = s + " Scala"
                      }
                    }
                    println("Hello". addSuffix)
             *利用隐式转换丰富类库功能
                    如果需要为一个类增加一个方法，
                    可以通过隐式转换来实现。
                    比如想为MySQL增加一个delete方法
   16.数据结构
          一.Scala同时支持可变集合和不可变集合，不可变集合从不可变，可以安全的并发访问。
            两个主要的包：
            不可变集合：scala.collection.immutable
            可变集合：  scala.collection.mutable
            Scala默认采用不可变集合，对于几乎所有的集合类，Scala都同时提供了可变和不可变的版本。
    17.
        *map：将集合中的每一个元素通过指定功能（函数）映射（转换）成新的结果集合
        * flatmap：flat即压扁，压平，扁平化，效果就是将集合中的
           每个元素的子元素映射到某个函数并返回新的集合
        *filter：将符合要求的数据(筛选)放置到新的集合中
        * 迭代器
           val iterator = List(1, 2, 3, 4, 5).iterator
           while (iterator.hasNext) {
             println(iterator.next())
           }
           或：
           for(enum <- iterator) {
             println(enum)
           }
       18高阶函数
            *作为参数的函数
                函数作为一个变量传入到了另一个函数中，
                那么该作为参数的函数的类型是：function1，
                    即：（参数类型） => 返回类型
            *匿名函数
                即没有名字的函数，
                可以通过函数表达式来设置匿名函数
                val triple = (x: Double) => 3 * x
                println(triple(3))
             *能够接受函数作为参数的函数，叫做高阶函数

                代码详见scalatest.sec.方法.MethodTest2（很重要）

        19类的初始化
             //_ 表示初始化
                  override var name: String = _
                  var age: Int = _

         20.注意
             class User {
               //申明属性，，scala给类声明属性，默认是私有的，但是底层
               //提供了公开的setter和getter方法
               var name:String = _
               //如果给属性增加private修饰符，name底层的setter和getter方法都是私有的
              private var age:Int = _
               //如果声明的属性使用val，那么属性是私有的，并且使用final修饰，底层只提供getter，而没有setter方法
               val  address:String = _


             }
          21.不可变集合和数组，增删查改都会创建新的集合。
            可变集合和数组，增删查改不会创建新的集合

            集合和数组都有一个aplly伴生对象，都可以直接 new List(1,2,3);

             队列一定是可变集合
             所以默认情况下只有队列是可变集合，其他都是不可变集合
             元组可以当做map集合中的键值对来使用

          22.在函数参数里不能直接省略为（_）,因为_含义太多，很难推断，默认当做function处理，不当做参数处理


          23.控制抽象
             //控制抽象，函数参数没有输入值，也没有返回值
              def test(f: =>Unit) ={
                f
              }

              test{
                print("hehe")
              }


              24模式匹配
                 val  (aa,bb,cc)=("11","22","33")
                 print(aa)

              25.scala所有的集合和数组输出都可以用 对象名.mkString(",")输出


              26.当scala  new class一个对象时，class里除了里面的方法不执行，剩下的都执行


              27scala定义在object里的方法和变量都是静态的

              28.object 和 trait不可以传参


              29.参数里的_只使用一次时，可以省略,方法里只有一个参数时，调用可以省略
                如println

              30.定义方法“=”可以省略，省略之后，无论方法体最后一行计算的结果是什么
                    都会被丢弃，返回Unit


             31.
               *递归函数要指定函数的返回值类型，要显示声明
               *函数的返回是函数时，要显示声明或者在最后一行用函数名f _

             32.flatMap =map + flatten


             33.tuple最多支持22个元素


             34.map集合里的元素是2元元组

             35.元组里没有foreach，要用tuple.productIterator

             36./**
                     * 8.高阶函数
                     *  1.函数的参数是函数
                     *  2.函数的返回是函数  --函数的返回是函数时，要显式声明函数的返回类型
                     *  3.函数的参数和返回都是函数
                     */

                  /**
                      * 9.柯里化函数
                      *  柯里化函数就是高阶函数的简化版
                      */

            37.样例类实现了apply方法，意味着你不需要使用new关键字就能创建该类对象
                case class People(name:String,age:Int)
                val p = People("mobin",22) //省略了new关键字

             38.    偏函数参考https://www.jianshu.com/p/5d3b5881ae13
               *由于偏函数继承自函数，因而，如果一个方法要求接收函数，那么它也可以接收偏函数。
               例如我们常常使用的map、filter等方法，就可以接收偏函数：
                   val sample = 1 to 10
                   sample map {
                       case x if x % 2 == 0 => x + " is even"
                       case x if x % 2 == 1 => x + " is odd"
                   }


                     在Twitter的Effetive Scala中，给出了一个使用map的编码风格建议：
                     //avoid
                     list map { item =>
                       item match {
                         case Some(x) => x
                         case None => default
                       }
                     }
                     //recommend
                     list map {
                       case Some(x) => x
                       case None => default
                     }


            从本质上讲，假设这个list的类型为List[Option[String]]，
            则前者传给map的其实是一个形如Option[String] => String的函数，
            后者则通过case语句创建了PartialFunction[Option[String], String](A => B)
             的实例传递给了map。

       *Scala中的Partial Function就是一个“残缺”的函数，就像一个严重偏科的学生，
           只对某些科目感兴趣，而对没有兴趣的内容弃若蔽履。Partial Function做不到以“偏”概全，
           因而需要将多个偏函数组合，最终才能达到全面覆盖的目的。
           所以这个Partial Function确实是一个“部分”的函数。
       *对比Function和Partial Function，更学术味的解释如下：
             Function:对给定的输入参数类型，函数可接受该类型的任何值 。
            换句话说，一个(Int) => String 的函数可以接收任意Int值，并返回一个字符串。
             Partial Function:对给定的输入参数类型，偏函数只能接受该类型的某些特定的值。
                一个定义为(Int) => String 的偏函数可能不能接受所有Int值为输入。
       *在Scala中，所有偏函数的类型皆被定义为PartialFunction[-A, +B]类型，
       PartialFunction[-A, +B]又派生自Function1。
       由于它仅仅处理输入参数的部分分支，因而它通过isDefineAt()来判断输入值是否应该由当前偏函数进行处理。

       *既然偏函数仅处理部分分支，自然可以与模式匹配结合起来。
        case语句从本质上讲就是PartialFunction的子类。当我们定义了如下值：
                 val p:PartialFunction[Int, String] = { case 1 => "One" }
        实际上就是创建了一个PartialFunction[Int, String]的子类，其中isDefineAt方法提供类似这样的实现：
            def isDefineAt(x: Int):Boolean = x == 1


            源码
             object PartialFunction {  //实际可以看出A=》B的一个匿名函数

                      def apply(x: A): B = f1.applyOrElse(x, f2)；

             }























spark总用zookeeper,但是可以用zookeeper配置高可用）
        概括：一种基于内存的快速，通用，可扩展的大数据分析引擎。
            由scala编写
        1.spark有三种运行模式：local本地运行模式,standalone模式和完全分布式模式
                (1)local模式(配置一台机器，直接解压就能用)

                (2)standalone模式(配置三台机器，Master+Slave构成的Spark集群，Spark运行在集群中。）

                (3)完全分布式模式（配置一台机器，然后让yarn实现分布式）
         2.用spark-shell打开spark的shell客户端，默认是本地模式
           spark-shell -master yarn 打开yarn模式

         3.spark有两个重要角色，Driver和Executor
            Driver（驱动器）作用：
                driver是main方法的进程。。它负责开发人员编写的用来创建SparkContext、创建RDD，
                以及进行RDD的转化操作和行动操作代码的执行。
               如果你是用spark shell，那么当你启动Spark shell的时候，系统后台自启了一个Spark驱动器程序，就是在Spark shell中预加载的一个叫作 sc的SparkContext对象。如果驱动器程序终止，那么Spark应用也就结束了。主要负责：
                1）把用户程序转为作业（JOB）
                2）跟踪Executor的运行状况
                3）为执行器节点调度任务
                4）UI展示应用运行状况
             Executor（执行器作用）：Spark Executor是一个工作进程，负责在 Spark 作业中运行任务，任务间相互独立。
                            Spark 应用启动时，Executor节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。
                            如果有Executor节点发生了故障或崩溃，Spark 应用也可以继续执行，
                            会将出错节点上的任务调度到其他Executor节点上继续运行。主要负责：
                                      1）负责运行组成 Spark 应用的任务，并将结果返回给驱动器进程；
                                      2）通过自身的块管理器（Block Manager）为用户程序中要求缓存的RDD提供内存式存储。
                                        RDD是直接缓存在Executor进程内的， 因此任务可以在运行时充分利用缓存数据加速运算。
        4.Spark内置模块
               *Spark Core：实现了Spark的基本功能，包含任务调度、内存管理、
                            错误恢复、与存储系统交互等模块。
                            Spark Core中还包含了对弹性分布式数据集
                            (Resilient Distributed DataSet，简称RDD)的API定义。
               *Spark SQL：Spark用来操作结构化数据的程序包。通过Spark SQL，我们可以使用 SQL或者Apache Hive版本的SQL方言(HQL)来查询数据。
                      Spark SQL支持多种数据源，比如Hive表、Parquet以及JSON等。
                *Spark Streaming：是Spark提供的对实时数据进行流式计算的组件。
                    提供了用来操作数据流的API，并且与Spark Core中的 RDD API高度对应。
                *Spark MLlib：提供常见的机器学习(ML)功能的程序库。
                   包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据 导入等额外的支持功能。
                *集群管理器：Spark 设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计 算。为了实现这样的要求，同时获得最大灵活性，Spark支持在各种集群管理器(Cluster Manager)上运行，
                    包括Hadoop YARN、Apache Mesos，以及Spark自带的一个简易调度 器，叫作独立调度器。

         5.spark rdd导入spark core包就行
            <dependencies>
                <dependency>
                    <groupId>org.apache.spark</groupId>
                    <artifactId>spark-core_2.11</artifactId>
                    <version>2.1.1</version>
                </dependency>
            </dependencies>
          6.spark rdd代码步骤
            //1.创建SparkConf并设置App名称
                val conf = new SparkConf().setAppName("WC")

            //2.创建SparkContext，该对象是提交Spark App的入口
                val sc = new SparkContext(conf)

                //3.使用sc创建RDD并执行相应的transformation和action
                sc.textFile(args(0)).flatMap(_.split(" ")).map((_, 1)).reduceByKey(_+_, 1).sortBy(_._2, false).saveAsTextFile(args(1))

            //4.关闭连接
                sc.stop()
           7.
             *local 所有计算都运行在一个线程当做，没有任何并行计算

             *local[K]:指定使用几个线程来运行计算，比如local[4]就是运行
            4个Worker线程。通常我们的cpu有几个核，就指定几个线程
              最大化利用cpu的计算能力

             *local[*]这种模式直接帮你按照cores来设置线程数了

           8.RDD（Resilient Distributed Dataset）叫做分布式数据集，
                是Spark中最基本的数据抽象。代码中是一个抽象类，
                它代表一个不可变、可分区、里面的元素可并行计算的集合。

           9.RDD的特点
                     RDD表示只读的分区的数据集，对RDD进行改动，
                      只能通过RDD的转换操作，由一个RDD得到一个新的RDD
                      RDDs之间存在依赖，RDD的执行是按照血缘关系延时计算的。
                      如果血缘关系较长，可以通过持久化RDD来切断血缘关系。
           10.RDD的属性

           * Internally, each RDD is characterized by five main properties:
            *
            *  - A list of partitions  一组分区（Partition），即数据集的基本组成单位;
                    （讲不同分区发给不同executor，体现并行运算）
            *  - A function for computing each split  一个计算每个分区的函数;
            *  - A list of dependencies on other RDDs  RDD之间的依赖关系（血统）
            *  - Optionally, a Partitioner
               for key-value RDDs (e.g. to say that the RDD is hash-partitioned)
                4)一个Partitioner，即RDD的分片函数;
            *  - Optionally, a list of preferred locations to compute each split on (e.g. block locations for
              一个列表，存储存取每个Partition的优先位置（preferred location）（意思是移动数据不如移动计算）
           11.RDD体现了控制抽象思想，将一段逻辑传进去

           12.算子：从认知心理学角度，解决问题其实是将问题的初始状态，通过
                一系列的操作（operate）算子对问题的状态进行转换，
                然后达到完成（解决）状态。

                Spark中所有的RDD方法都称作算子，算子被分为两大类：转换算子和行动算子



            13.RDD的特点
                    *分区：RDD逻辑上是分区的，每个分区的数据是抽象存在的，
                       计算的时候会通过一个compute函数得到每个分区的数据。
                     *只读：RDD是只读的，要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。

                      *依赖：RDDs通过操作算子进行转换，转换得到的新RDD包含了从其他RDDs衍生所必需的信息，
                           RDDs之间维护着这种血缘关系，也称之为依赖(血统)
                      *缓存：如果在应用程序中多次使用同一个RDD，可以将该RDD缓存起来，
                         该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，
                       在后续其他地方用到该RDD的时候，会直接从缓存处取而不用再根据血缘关系计算
                       *CheckPoint：虽然RDD的血缘关系天然地可以实现容错，
                          当RDD的某个分区数据失败或丢失，可以通过血缘关系重建。

             14RDD的创建
                在Spark中创建RDD的创建方式可以分为三种：从集合中创建RDD；从外部存储创建RDD；从其他RDD创建。
                    *从集合中创建RDD，Spark主要提供了两种函数：parallelize和makeRDD
                         //1.从集合（内存）中创建,makeRdd底层实现就是parallelize,没有任何区别
                            val rddArray: RDD[Int] = sc.makeRDD(Array(1,2,3,4))  //Array用List也行
                            rddArray.foreach(print(_))  //没有collect也可以直接输出结果

                          //2.从集合内存中创建
                            val rddList: RDD[Int] = sc.parallelize(List(1,2,3,4))
                            rddList.collect().foreach(print(_))   //collect
                      *从外部存储中创建
                            //如果是hdfs写hdfs://hadop102:9000/
                            val lines: RDD[String] = sc.textFile("in/word1")  //读文件一行一行是String类型
                            lines.collect().foreach(println(_))

         15.spark分区和生成几个文件的说明
               （1） spark有几个分片（分区）就有几个并行度，也就是给跑的程序几个核数，local[*]是当前
               （2）机器有几个核数，那么默认就有几个分区。后期也可以在创建RDD时（makeRDD和textFile里设置）
                        *makeRDD:
                            第二个参数分片有默认值，就是当前设置的分片数

                        *testFile:
                            （1）第二个参数分片数有默认值，
                           （2）读取文件时，传递的分区参数为最小分区数时，
                          不一定就是这个分区数，取决于hadoop读取文件时
                          的分片原则
               （3）本地模式官方并行度默认值（默认并行度=总核心数）
                 override def defaultParallelism(): Int =
                    scheduler.conf.getInt("spark.default.parallelism", totalCores)
               当自己设置的分片数大于电脑总的核心数时，任然可以按照自己设置的核心数运行（3个）
               val rddArray: RDD[Int] = sc.makeRDD(Array(1,2,3,4),3)
              （3）总结：分片数=并行度=当前程序设置的核心数=task个数=分区数=输出文件个数

              （4）日志分析
                    /**
                      *   MakeRDD（）方法
                        def makeRDD[T: ClassTag](
                          seq: Seq[T],
                           numSlices: Int = defaultParallelism): RDD[T] = withScope { //这里是默认并行度，注意和textFile区别
                          parallelize(seq, numSlices)
                         }
                      *
                      * local（并行度，核数）为1时
                          *     Got job 0 (saveAsTextFile at RDD分片.scala:14) with 1 output partitions
                          *  Adding task set 0.0 with 1 tasks
                          *    Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at saveAsTextFile
                          *  Running task 0.0 in stage 0.0 (TID 0)
                          *   Finished task 0.0 in stage 0.0 (TID 0). 1012 bytes result sent to driver
                          *  Finished task 0.0 in stage 0.0 (TID 0) in 411 ms on localhost (executor driver) (1/1)
                          *   Removed TaskSet 0.0, whose tasks have all completed, from pool
                      *
                      *
                      *   分片数（并行度，核数）为3时
                      *   Got job 0 (saveAsTextFile at RDD分片.scala:26) with 3 output partitions
                      *      Submitting 3 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at saveAsTextFile at RDD分片
                      *   Adding task set 0.0 with 3 tasks
                      *
                      *      TODO 开启了三个task
                      *    Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 5892 bytes)
                      *      Finished task 0.0 in stage 0.0 (TID 0). 1012 bytes result sent to driver
                      *   Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 5892 bytes)
                      *       Finished task 1.0 in stage 0.0 (TID 1). 925 bytes result sent to driver
                      *  Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 5896 bytes)
                      *     Finished task 2.0 in stage 0.0 (TID 2) in 59 ms on localhost (executor driver)
                      *
                      *     以上结果可以知道三个并行度会开启一个job，三个task，三个partition（默认都是从0开始）
                      *     和hadoop异同。hadoop有多少个分片就有多少个maptask，每一个mapper类就是一个maptask。
                      *       hadoop的inputformat分片机制取决于文件得个数和每一个文件的大小（0-128M）
                      *
                      *   总结：分片数=并行度=当前程序设置的核心数=task个数=分区数=输出文件个数
                      *
                      *  textFile（）方法
                         def textFile(
                            path: String,
                            minPartitions: Int = defaultMinPartitions  //这个参数是最小分区数，注意和makeRDD区别
                         )
                          : RDD[String] = withScope {
                          assertNotStopped()
                          hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text],
                            minPartitions).map(pair => pair._2.toString).setName(path)   //这里说明还要取决于hadoopFile的分片机制
                                                                                        //主要是minPartitions为2时会有区别
                      }

                      *运行日志：
                        默认并行度：
                        def defaultMinPartitions: Int = math.min(defaultParallelism, 2)
                               defaultParallelism这个就是makeRDD里的默认并行度，但是local[*]设置大于2时，默认取2
                         override def defaultParallelism(): Int =
                          scheduler.conf.getInt("spark.default.parallelism", totalCores)
                      *  一个并行度
                      *   submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at saveAsTextFil
                      *   Adding task set 0.0 with 1 tasks
                      *   Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 6000 bytes)
                      *    Finished task 0.0 in stage 0.0 (TID 0). 1245 bytes result sent to driver
                      *
                      *  三个并行度(和MakeRDD一样，这里省略)
                      *    Submitting 3 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at saveAs
                      *
                      *
                      */
            16.算子
                 转换算子单value:
                        *map对每一个集合里的元素操作组成新的集合
                        *mapPartitions对每一个分区内的集合操作组成新得集合
                        *mapPartitionsWithIndex对每一个分区内的集合操作组成新得集合
                            并且附带将分区的下标带过来
                        *flatmap对每一个集合里的元素的子元素操作形成新的集合
                        *groupBy根据key进行分组
                        *filter 过滤操作成为新的集合
                        *sample对集合里的元素进行抽样成为新的集合
                             val sampleRdd: RDD[Int] = rdd.sample(true,0.2,2)
                               参数说明
                                1)第一个参数withReplacement为true表示有放回，为false表示无放回，
                                2)fraction表示抽取的数量
                                3)第三个参数seed可以不传，不传的话是真正的随机数，
                                   每次产生都不一样,传入后，是伪随机数，
                                   每次只要参数相同，产生的数据都是一样的
                        *distinct 对集合相同元素去重，有shuffle过程
                        *coalesce 减少分区数，默认没有shuffle
                        *glom将每一个分区形成一个数组，形成新的RDD类型时RDD[Array[T]]
                        *repartition 在当前rdd上重新分区转换成新的rdd，和coalesce不同的是，
                            此方法有shuffle过程
                         *sortBy是按照处理后的数据结果排序，默认为正序，false为倒序，true为正序

                 转换算子双value:
                   *union 求两个集合的并集，会重复
                   *subtract 求两个集合的差集
                   *intersection 求两个集合的交集
                   *cartesian 笛卡尔积

                 转换算子key-value：
                    key-value的方法不在rdd类里，value类型的方法都在rdd类里




            17spark是按行读取的


            18flatMap解读 对于val list =List(List(T)),list.flatMap(x=>b),x指List，b指
                对T的操作


            19
                //coalesce(1) 只会输出一个文件，reduce
                lines.coalesce(1).saveAsTextFile("out/hadoop/lines06")

             20.模式匹配语法中，采用match关键字声明，
                 每个分支采用case关键字进行声明，如果匹配不成功，
                 继续执行下一个分支进行判断。如果所有case都匹配，
                 那么会执行case _ 分支，类似于Java中default语句。
                  如果没有任何模式匹配成功，那么会抛出MatchError。
                    每个case中，不用break语句。

             21.只有数据分区好后有打乱顺序才有shuffle，否则没有shuffer

             22.rdd.partitions.size获取当期RDD的分区个数，
               rdd.partitions获取当前RDD的分区

            23.一个分区划分为一个任务（Task），一个任务会分配到Executor里执行


            24.spark里的ui界面，会在启动后日志里面，自己去找，一般端口号是4040
                    spark-shell启动后的jps也是spark-submit

            25.spark rdd只有遇到行动算子后才会触发操作
flink总结：
     1.Flink是一种框架和分布式处理引擎，用于对无界和有界数据
        流进行有状态计算。Flink被设计在所有常见的集群汇总运行，
        以内存速度和任意闺蜜来执行计算。
     2.spark是事件驱动型，事件驱动应用是一类具有状态的应用，
      它从一个或多个事件流提取数据，并根据到来的事件触发计算、状态更新或其他外部动作。
      比较典型的就是以kafka为代表的消息队列几乎都是事件驱动型应用。
      3.SparkStreaming是微批次，而Flink是流处理
            *批处理的特点是有界、持久、大量，
                非常适合需要访问全套记录才能完成的计算工作，一般用于离线统计。
             *流处理的特点是无界、实时,  无需针对整个数据集执行操作，
               而是对通过系统传输的每个数据项执行操作，一般用于实时统计。
             * 在spark的世界观中，一切都是由批次组成的，
                离线数据是一个大批次，而实时数据是由一个一个无限的小批次组成的。
             *  而在flink的世界观中，一切都是由流组成的，离线数据是有界限的流，
                实时数据是一个没有界限的流，这就是所谓的有界流和无界流。
             *无界数据流：无界数据流有一个开始但是没有结束，它们不会在生成时终止并提供数据，
               必须连续处理无界流，也就是说必须在获取后立即处理event。
              *有界数据流：有界数据流有明确定义的开始和结束，有界流的处理也称为批处理。

       4.Flink分层api
         *大多数应用是针对核心API（Core APIs） 进行编程，
            比如DataStream API（有界或无界流数据）以及DataSet API（有界数据集)
         *Table API 是以表为中心的声明式编程，其中表可能会动态变化（在表达流数据时）。Table API遵循（扩展的）关系模型：表有二维数据结构（schema）（类似于关系数据库中的表），同时API提供可比较的操作，例如select、project、join、group-by、aggregate等。
           Table API程序声明式地定义了什么逻辑操作应该执行，
            而不是准确地确定这些操作代码的看上去如何 。你可以在表与 DataStream/DataSet 之间无缝切换，
             以允许程序将 Table API 与 DataStream 以及 DataSet 混合使用.
             Flink提供的最高层级的抽象是 SQL
         5.支持有状态计算，将算子的中间结果保存在内存或者文件系统中

         6.支持exactly-once语义
            *At Most once 对于一条message,receiver最多收到一次(0次或1次).

            *At Least once: 对于一条message,receiver最少收到一次(1次及以上).

            *Exactly once: 对于一条message,receiver确保只收到一次

          7.编程
             导入以下maven包就行
             <dependencies>
                     <dependency>
                         <groupId>org.apache.flink</groupId>
                         <artifactId>flink-scala_2.11</artifactId>
                         <version>1.7.0</version>
                     </dependency>

                     <!-- https://mvnrepository.com/artifact/org.apache.flink/flink-streaming-scala -->
                     <dependency>
                         <groupId>org.apache.flink</groupId>
                         <artifactId>flink-streaming-scala_2.11</artifactId>
                         <version>1.7.0</version>
                     </dependency>
                 </dependencies>


                代码：
                   *批处理
                    def main(args: Array[String]): Unit = {

                      //构造执行环境
                      val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment
                      //读取文件
                      val input = "file:///d:/temp/hello.txt"
                      val ds: DataSet[String] = env.readTextFile(input)
                      // 其中flatMap 和Map 中  需要引入隐式转换
                      import org.apache.flink.api.scala.createTypeInformation
                      //经过groupby进行分组，sum进行聚合
                      val aggDs: AggregateDataSet[(String, Int)] = ds.flatMap(_.split(" ")).map((_, 1)).groupBy(0).sum(1)
                      // 打印
                      aggDs.print()

                    }

                     *批处理
                        def main(args: Array[String]): Unit = {
                            //从外部命令中获取参数
                            val tool: ParameterTool = ParameterTool.fromArgs(args)
                            val host: String = tool.get("host")
                            val port: Int = tool.get("port").toInt

                            //创建流处理环境
                            val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment
                            //接收socket文本流
                            val textDstream: DataStream[String] = env.socketTextStream(host,port)
                           // flatMap和Map需要引用的隐式转换
                            import org.apache.flink.api.scala._
                           //处理 分组并且sum聚合
                            val dStream: DataStream[(String, Int)] = textDstream.flatMap(_.split(" ")).filter(_.nonEmpty).map((_,1)).keyBy(0).sum(1)
                           //打印
                            dStream.print()

                            env.execute()
                          }

          8.Flink和Spark两个和重要角色区别
                (spark里：driver 	executor)
                (Flink里：jobmanager		taskmanager)

          9.Flink有standalone和yarn两种模式
              * standalone模式(解压后配置jobmanager和taskmanager)
                    启动方式
                    ./start-cluster.sh
                    提交任务
                    ./flink run -c wc.WordCount2  /home/lisi/FlinkTest-1.0-SNAPSHOT.jar  --input /home/lisi/word.txt --output /home/lisi/flinkoutput/output.csv

              *yarn模式
              （打开yarn-session，启动时加上启动方式即可,我认为也只是只用配置一台）









